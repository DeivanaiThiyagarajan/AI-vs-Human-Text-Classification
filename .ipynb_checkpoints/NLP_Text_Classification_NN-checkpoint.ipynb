{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb628a3-13e5-40fc-bdc5-b197d739e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade1c9c6-ec48-45d8-b92b-e24fd98f3c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a08804f-52e4-4268-a545-2b37253bf5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the data from the path\n",
    "data_path = \"DataSets/Processed_Data_small.csv\"  \n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885d2253-691d-48c4-bb28-fddc5d7a11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null records\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c6fe09-a543-4bea-b01c-3c3cd88cecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segregate text data, numrical data and labels\n",
    "text_data = df['text']\n",
    "numerical_features = df[['avg_len_sentences', 'punctuations_count','readability_score','words_count','stop_word_ratio']]\n",
    "labels = df['generated']  # 1 for AI-generated, 0 for human-generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b64aa9e-193f-4bd6-9990-bf0afa456c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the numerical features to be in the range of 0 to 1\n",
    "scaler = MinMaxScaler()\n",
    "numerical_features = scaler.fit_transform(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724e813b-c715-4e56-97a9-e246cf55dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data as 70% training data and 30% test data\n",
    "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(\n",
    "    text_data, numerical_features, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d937b112-03b8-43eb-b90a-1ea3f1ac9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TF-IDF to vectorize text with max features as 1000 and ngram range (1,2)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2)) \n",
    "X_train_text_tfidf = tfidf_vectorizer.fit_transform(X_train_text).toarray()\n",
    "X_test_text_tfidf = tfidf_vectorizer.transform(X_test_text).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93adf91c-8895-4912-b262-b6138ae9d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class and methods to convert each feature to torch tensor\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_features, numerical_features, labels):\n",
    "        self.text_features = torch.tensor(text_features, dtype=torch.float32)\n",
    "        self.numerical_features = torch.tensor(numerical_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_features[idx], self.numerical_features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26b1117-b987-4521-8f1e-d08b9257c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train and test data separately\n",
    "train_dataset = TextDataset(X_train_text_tfidf, X_train_num, y_train)\n",
    "test_dataset = TextDataset(X_test_text_tfidf, X_test_num, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf85f850-db2a-4ad8-a613-2e2f26ec26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add layers to the NN model with 3 linear layers using ReLU activation function and dropout of 30%\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, text_input_dim, num_input_dim, num_classes=2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(text_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.num_fc = nn.Sequential(\n",
    "            nn.Linear(num_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, text_input, num_input):\n",
    "        text_output = self.text_fc(text_input)\n",
    "        num_output = self.num_fc(num_input)\n",
    "        combined = torch.cat((text_output, num_output), dim=1)\n",
    "        output = self.combined_fc(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4194aadf-bc15-459f-973b-b2a7ce9930e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (text_fc): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (num_fc): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (combined_fc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the model to use GPU\n",
    "model = TextClassificationModel(text_input_dim=X_train_text_tfidf.shape[1], num_input_dim=X_train_num.shape[1])\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd06206-3f98-4b41-a81f-dea38a6b8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add optimizer and loss functions to the neural netowrks\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0089b9-0053-470a-81b1-d13e0b83dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model by converting every feature to utilize GPU\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for text_features, numerical_features, labels in train_loader:\n",
    "            text_features, numerical_features, labels = text_features.to(device), numerical_features.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(text_features, numerical_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fcc6b2a-ce65-481e-bd28-9386f03d272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.2582, Accuracy: 88.8126\n",
      "Epoch [2/20], Loss: 0.2029, Accuracy: 91.4647\n",
      "Epoch [3/20], Loss: 0.1766, Accuracy: 92.6312\n",
      "Epoch [4/20], Loss: 0.1522, Accuracy: 93.7523\n",
      "Epoch [5/20], Loss: 0.1299, Accuracy: 94.7856\n",
      "Epoch [6/20], Loss: 0.1097, Accuracy: 95.6241\n",
      "Epoch [7/20], Loss: 0.0943, Accuracy: 96.3490\n",
      "Epoch [8/20], Loss: 0.0819, Accuracy: 96.8538\n",
      "Epoch [9/20], Loss: 0.0719, Accuracy: 97.2715\n",
      "Epoch [10/20], Loss: 0.0645, Accuracy: 97.5930\n",
      "Epoch [11/20], Loss: 0.0575, Accuracy: 97.8474\n",
      "Epoch [12/20], Loss: 0.0530, Accuracy: 98.0464\n",
      "Epoch [13/20], Loss: 0.0498, Accuracy: 98.1875\n",
      "Epoch [14/20], Loss: 0.0463, Accuracy: 98.2997\n",
      "Epoch [15/20], Loss: 0.0424, Accuracy: 98.4541\n",
      "Epoch [16/20], Loss: 0.0405, Accuracy: 98.5387\n",
      "Epoch [17/20], Loss: 0.0374, Accuracy: 98.6613\n",
      "Epoch [18/20], Loss: 0.0367, Accuracy: 98.6909\n",
      "Epoch [19/20], Loss: 0.0339, Accuracy: 98.7953\n",
      "Epoch [20/20], Loss: 0.0339, Accuracy: 98.8006\n"
     ]
    }
   ],
   "source": [
    "#call the train function with all necessary data, model and optimizers\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2c5245-a70b-4eec-a411-445fa0d0b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluat the model for test data\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for text_features, numerical_features, labels in test_loader:\n",
    "            text_features, numerical_features, labels = text_features.to(device), numerical_features.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(text_features, numerical_features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5205f5f8-54af-4ee9-8706-a2c0acfb6703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[54715  5285]\n",
      " [ 5367 54589]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     60000\n",
      "           1       0.91      0.91      0.91     59956\n",
      "\n",
      "    accuracy                           0.91    119956\n",
      "   macro avg       0.91      0.91      0.91    119956\n",
      "weighted avg       0.91      0.91      0.91    119956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report for the test data\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred, y_true = evaluate_model(model, test_loader, device)\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971bea8-7c73-4954-aa15-30ed92722604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model as a pickle file\n",
    "def save_model_as_pickle(model, file_path=\"ai_vs_human_text_model.pkl\"):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved as pickle to {file_path}\")\n",
    "\n",
    "save_model_as_pickle(model, file_path=\"ai_vs_human_text_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703224a9-843f-4391-8e13-4f65f28de441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path=\"trained_model.pth\"):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "save_model(model, file_path=\"ai_vs_human_text_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335480c9-0535-4b71-9395-959a96e9bfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
